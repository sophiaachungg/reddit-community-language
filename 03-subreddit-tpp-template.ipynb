{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cae1300-3698-4a99-9b18-77c063f1c770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sophiachung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sophiachung/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44c8774-fb60-43a7-8d9e-ca47f73aee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# function to remove all types of URLs\n",
    "def remove_urls(text):\n",
    "    # regex pattern to match URLs starting with [https://...] or (https://...)\n",
    "    # handles [https://...] and (https://...)\n",
    "    url_pattern = r'\\[https?://[^\\]]+\\]|\\(https?://[^\\)]+\\)'  \n",
    "    \n",
    "    # regex pattern to match URLs without http/https, just domains\n",
    "    domain_pattern = r'\\b\\w+\\.com\\b|\\b\\w+\\.co\\b|\\b\\w+\\.store\\b|\\b\\w+\\.org\\b|\\b\\w+\\.edu\\b|\\b\\w+\\.net\\b|\\b\\w+\\.us\\b|\\b\\w+\\.uk\\b|\\b\\w+\\.eu\\b|\\b\\w+\\.ca\\b'\n",
    "    \n",
    "    # combine both patterns and remove all matches\n",
    "    combined_pattern = f'{url_pattern}|{domain_pattern}'\n",
    "    \n",
    "    cleaned_text = re.sub(combined_pattern, '', text)  # replace the URL with an empty string\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28edcf7e-a7c8-4b3f-ac1d-36848b44afee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6073ded7-6b29-40f8-9821-4bf065a62ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the function to remove apostrophes, specifically\n",
    "def remove_punctuation(text):\n",
    "        punctuationfree=\"\".join([i for i in text if i not in string.punctuation[6]])\n",
    "        return punctuationfree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1990231-2e25-4511-af27-8d2dc77813d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    tokens = re.split(r'\\W+',text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aee7c918-714b-4ab6-9730-73d9db943fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9689f023-088b-4ef8-b917-ef4e3ca920d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c875f791-984e-47c1-b750-4d1f009c4603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(subreddit_name):\n",
    "    path = 'tidied/r_' + subreddit_name + '_tidied.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[['ID', 'Created On', 'Post Text']]\n",
    "    df = df.rename(columns = {'Post Text': 'text'})\n",
    "    \n",
    "    # applying function to the column with the Reddit text\n",
    "    df['no_urls'] = df['text'].apply(lambda x: remove_urls(x))\n",
    "    \n",
    "    # applying the function to remove punctuation \n",
    "    df['clean_msg']= df['no_urls'].apply(lambda x:remove_punctuation(x))\n",
    "   \n",
    "    # lowering text\n",
    "    df['msg_lower']= df['clean_msg'].apply(lambda x: x.lower())\n",
    "    \n",
    "    # applying tokenization to text\n",
    "    df['msg_tokenized']= df['msg_lower'].apply(lambda x: tokenization(x))\n",
    "\n",
    "    # remove stopwords\n",
    "    df['no_stopwords']= df['msg_tokenized'].apply(lambda x:remove_stopwords(x))\n",
    "    \n",
    "    # applying lemmatization to text\n",
    "    df['msg_lemmatized']=df['no_stopwords'].apply(lambda x:lemmatizer(x))\n",
    "\n",
    "    # save new pre-processed dataset\n",
    "    output_file_path = 'tpp/r_' + subreddit_name + '_tpp.csv'\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(\"\\ntext-preprocessed dataset saved to:\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20fca0cb-a575-4e17-85db-4ed60357276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "text-preprocessed dataset saved to: tpp/r_adops_tpp.csv\n"
     ]
    }
   ],
   "source": [
    "# test full function\n",
    "subreddit_name = 'adops'\n",
    "text_preprocessing(subreddit_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc77bd99-7411-4075-af10-a010d3cd22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['adops'] # create list of subreddits to pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a1b4454-e7aa-4619-b6c4-ae51d7894527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "text-preprocessed dataset saved to: tpp/r_adops_tpp.csv\n"
     ]
    }
   ],
   "source": [
    "for subreddit in subreddits: \n",
    "    name = subreddit.lower()\n",
    "    text_preprocessing(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7882bb7f-a145-4925-88fa-c6f2bfedc3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying something\n",
    "def text_preprocessing(subreddit_name):\n",
    "    path = 'tidied/r_' + subreddit_name + '_tidied.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[['ID', 'Created On', 'Post Text']]\n",
    "    df = df.rename(columns = {'Post Text': 'text'})\n",
    "    \n",
    "    # Applying function to the column with the Reddit text\n",
    "    df['no_urls'] = df['text'].apply(lambda x: remove_urls(x))\n",
    "   \n",
    "    # Lowering text\n",
    "    df['msg_lower']= df['no_urls'].apply(lambda x: x.lower())\n",
    "    \n",
    "    # Applying tokenization to text\n",
    "    df['msg_tokenized']= df['msg_lower'].apply(lambda x: tokenization(x))\n",
    "\n",
    "    # Remove stopwords\n",
    "    df['no_stopwords']= df['msg_tokenized'].apply(lambda x:remove_stopwords(x))\n",
    "    \n",
    "    # Applying lemmatization to text\n",
    "    df['msg_lemmatized']=df['no_stopwords'].apply(lambda x:lemmatizer(x))\n",
    "\n",
    "    # save new pre-processed dataset\n",
    "    output_file_path = 'tpp/r_' + subreddit_name + '_tpp.csv'\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(\"\\ntext-preprocessed dataset saved to:\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7f993435-7e5f-4a68-bc37-00efd2470f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "text-preprocessed dataset saved to: tpp/r_roborock_tpp.csv\n"
     ]
    }
   ],
   "source": [
    "# test full function\n",
    "subreddit_name = 'roborock'\n",
    "text_preprocessing(subreddit_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3c1e8-e73a-4a1e-81f7-99fda67715ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
